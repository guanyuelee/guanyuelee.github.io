---
title: "High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition"
collection: publications
permalink: /publication/aaai2021
excerpt: ''
date: 2021-2-1
venue: 'AAAI'
---
[[Download paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17017), [[Github]](https://github.com/guanyuelee/PmSFC)

Problem Describtion
======
Generative Adversarial Networks (GANs) have achieved considerable success in high fidelity image synthesis and downstream applications, such as [PGGAN](), [StyleGAN](), [BigGAN]() and so on. But GAN has no way to inference for the latent code, which leads to the development of GAN inversion in these years. GAN inversion aims to reverse a target image back to a code in the latent space such that the inverted code can be as close to the target image as possible. 

Formally speaking, given a pre-trained generator $G$ and a target image $x$, we are required to find an optimal latent code $z$, such that $x = G(z)$. In this paper, we mainly discuss the PGGAN-like or BigGAN-like pretrained generator which doesn't include style-like modules like StyleGAN. The target code $z$ could be optimized by Gradient Descent: 

$$\min_{z} \|x - G(z)\|^2_2 + \|V(x) - V(G(z))\|_1, \tag{1}$$ 

where $V$ denotes a pre-trained network for feature extraction, like [VGG](). Equation $(1)$ can be efficiently optimized by [SGD]() or [Adam](). We show some inversion results from [Bau D., et al]() in Figure 1. From the results, we can see that inversion result produces reasonable reconstruction because inversion images has similar semantical structure with the target images, but the inversion results are hardly satisfactory. You can refer to paper ([Bau D., et al]()) for their deeper analysis on what GAN cannot generate. 

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../images/papers/aaai2021/naive_reconstruction.png">
    <br>
    <div style="color:orange;
    display: inline-block;
    color: black;
    padding: 2px;">
    Figure 1. Inversion results from Bau D., et al. 
    </div>
</center>

As reported in [Gu J., et al](), the expressiveness of a single latent code is less satisfactory for the case where the target image is out of the distribution of the GANâ€™s training data. In their model which is named mGANprior, multiple latent codes are used, and the corresponding intermediate feature maps are combined to improve the inversion performance. Formally speaking, they split the generator $G$ into two subnetworks along $l$-th layer: $G^l_1$ and $G^l_2$. The objective is to determine a set of latent codes $\textbf{z} = {z_1, z_2, \cdots, z_K}$ and a set of weighting vectors $\textbf{w} = {w_1, w_2, \cdots, w_K}$ for image reconstruction as follows:

$$\tilde{x}=G^l_2\left( \sum_{k=1}^{K}G_1^l(z_k)\odot w_k \right),\tag{2}$$
where the dimension of $w_k$ is the same as the number of channels, and the operation $\odot$ represents channel-wise multiplication.  To ensure the consistency between the original input and the reconstruction at both low and high levels, the optimization problem is defined as follows:

$$\min_{\textbf{z},\textbf{w}} \|x - \tilde{x}\|^2_2 + \|V(x) - V(\tilde{x})\|_1. \tag{3}$$ 
The most different part between equation $(1)$ and $(3)$ are the parameters to be optimized. In eqation $(1)$, only 1 latent code is optimized, while there are $K$ latent codes and $K$ mixing weights to be optimized in equation $(3)$. We show our implementation of mGANprior in Figure 2. We can see that Gu J., et al only regress to original image in the pixel-wise level instead of in semantic level, and also the results are also blurry. 

<table>
    <tr>
        <td>
            <center>
            <img src="../images/papers/aaai2021/multicode_reconstruction_celebahq.png" width=300>
            <br>
            <div style="color:orange;
            display: inline-block;
            color: black;
            padding: 2px;">
            (a) CelebA-HQ results
            </div>
            </center>
        </td>
        <td>
            <center>
            <img src="../images/papers/aaai2021/multicode_reconstruction_lsun.png" width=300>
            <br>
            <div style="color:orange;
            display: inline-block;
            color: black;
            padding: 2px;">
            (b) LSUB-Bedroom results
            </div>
            </center>
        </td>
    </tr>
</table>
<center>
<div style="color:orange;
    display: inline-block;
    color: black;
    padding: 2px;">
    Figure 2. Inversion results comparison between Gu J., et al (right) and original images (left). We use the official published implementation. 
    </div>
</center>

Our Ideas
======
Now let's look at our ideas to handle this problem. Firstly we use 

Subspace
------

PmSFC
------


Experiments
======


This paper is about the number 1. The number 2 is left for future work.

Recommended citation: Li, G., Jiao, Q., Qian, S., Wu, S., & Wong, H. S. (2021, May). High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition. <i>In Proceedings of the AAAI Conference on Artificial Intelligence </i> (Vol. 35, No. 9, pp. 8366-8374). 