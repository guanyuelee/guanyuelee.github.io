---
title: "High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition"
collection: publications
permalink: /publication/aaai2021
excerpt: ''
date: 2021-2-1
venue: 'AAAI'
---
[[Download paper]](https://ojs.aaai.org/index.php/AAAI/article/view/17017), [[Github]](https://github.com/guanyuelee/PmSFC)

Problem Describtion
======
Generative Adversarial Networks (GANs) have achieved considerable success in high fidelity image synthesis and downstream applications, such as [PGGAN](), [StyleGAN](), [BigGAN]() and so on. But GAN has no way to inference for the latent code, which leads to the development of GAN inversion in these years. GAN inversion aims to reverse a target image back to a code in the latent space such that the inverted code can be as close to the target image as possible. 

Formally speaking, given a pre-trained generator $G$ and a target image $x$, we are required to find an optimal latent code $z$, such that $x = G(z)$. In this paper, we mainly discuss the PGGAN-like or BigGAN-like pretrained generator which doesn't include style-like modules like StyleGAN. The target code $z$ could be optimized by Gradient Descent: 

$$\min_{z} \|x - G(z)\|^2_2 + \|V(x) - V(G(z))\|_1, \tag{1}$$ 

where $V$ denotes a pre-trained network for feature extraction, like [VGG](). Equation $(1)$ can be efficiently optimized by [SGD]() or [Adam](). We show some inversion results from [Bau D., et al]() in Figure 1. 

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="../images/papers/aaai2021/naive_reconstruction.png">
    <br>
    <div style="color:orange;
    display: inline-block;
    color: black;
    padding: 2px;">
    Figure 1. Inversion results from Bau D., et al. 
    </div>
</center>

Related Work
======

Our Ideas
======

Experiments
======


This paper is about the number 1. The number 2 is left for future work.

Recommended citation: Li, G., Jiao, Q., Qian, S., Wu, S., & Wong, H. S. (2021, May). High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition. <i>In Proceedings of the AAAI Conference on Artificial Intelligence </i> (Vol. 35, No. 9, pp. 8366-8374).